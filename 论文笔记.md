# Longformer

&emsp;&emsp;原代码：https://github.com/allenai/longformer
&emsp;&emsp;Transformer组件实现：https://github.com/huggingface/transformers/blob/master/src/transformers/models/longformer/modeling_longformer.py

## Motivation
&emsp;&emsp;传统Transformer-based模型在处理长文本时每一个token都需要与其他所有token进行交互，无论是空间还是时间复杂度都高达$O(n^2)$。为了解决这个问题，之前有些工作是将长文本切分为若干个较短的Text Segment，然后逐个处理，例如Transformer-XL。但这会导致不同的Text Segment之间无法进行交互，因而必然存在大量的information loss（信息丢失）。当然，我们也可以通过添加一些其它机制来加强Text Segment之间的交互，但这种新机制实现起来要么很复杂，要么是task-specific的，通用性不强

&emsp;&emsp;本文提出的Longformer，改进了Transformer传统的self-attention机制。具体来说，每一个token只对固定窗口大小附近的token进行local attention（局部注意力）。并且Longformer针对具体任务，在原有local attention的基础上增加了一种global attention（全局注意力）

&emsp;&emsp;Longformer在两个字符级语言建模任务上都取得了SOTA的效果。并且作者用Longformer的attention方法继续预训练RoBERTa，训练得到的语言模型在多个长文档任务上进行fine-tune后，性能全面超越RoBERTa。

## New Attention mechanism 
### 1.Sliding window（滑窗）
&emsp;&emsp;对于每一个token，只对其附近的$w$个token计算attention，计算复杂度与文本序列长度成线性关系，即$O(n*w)$ 。作者认为，根据应用任务的不同可以对Transformer每一层施以不同的窗口大小 ，对模型表示能力可能有潜在帮助。
<center><img src="1.PNG"  style="zoom:30%;" width="70%"/></center>


### 2. Dilated sliding window（空洞滑窗） 
&emsp;&emsp;在对每一个进行token编码时，普通滑窗机制只能考虑到长度$w$的上下文。作者进一步提出空洞滑窗机制，在不增加计算负荷的前提下，拓宽模型“视场”。其做法借鉴了CV中空洞卷积。如下图所示，在滑动窗口中，被attend到的两个相邻token之间会存在大小为d的间隙。当transformer的层数为l时，则视场范围可达到 。实验表明，由于考虑了更加全面的上下文信息，膨胀滑窗机制比普通的滑窗机制表现更佳。
<center><img src="2.PNG"  style="zoom:30%;" width="70%"/></center>

### 3.Global + Sliding window（融合全局信息的滑窗）
&emsp;&emsp;我们知道Bert一类的语言模型在应用于具体任务时，实现方式略有不同。比如，对于文本分类任务，我们会在文本序列前添加[CLS]这一特殊token；而对于QA类任务，则会将问题与文本进行拼接后输入。在Longformer中，作者也希望能够根据具体任务的不同，在local attention的基础上添加少量的global attention。比如，在分类任务上就会在[CLS]处添加一个global attention，而在QA任务上会对question中的所有token添加global attention。如下图所示，对于添加了global attention的token，我们对其编码时要对整个序列做attention。并且，编码其他所有token时，也都要attend到它。
<center><img src="3.PNG"  style="zoom:30%;" width="70%"/></center>

## 基于Transformer组件复现论文指标
### 一、Wiki-Hop
#### 数据预处理
&emsp;&emsp;模型输入为四部分：
* candidate_ids: <s> [question] question token ids [/question] [ent] candidate 1 token ids [/ent] [ent] candidate 2 ids ... [/ent]
* support_ids：</s> document 1 token ids </s> </s> document 2 ids </s> ... </s> document M ids </s>
* predicted_indices：由于在candidate_ids中我们用 [ent] 与 [/ent] 分隔了多个候选项，则predicted_indices中为各 [ent] 的位置
* answer_index

&emsp;&emsp;在构建Dataloader时，为了节省现存，bs取1。

#### 结果
&emsp;&emsp;预训练模型取：allenai/longformer-large-4096与原文中一致
|  评价指标 | 原论文 | 使用Transformers实现 |
|  :--:  |  :--:   |  :--:   |
| ACC | 81.9 | 表头  |
&emsp;&emsp;见Wikihop.py
